\chapter{Related Work}\label{sec:related}

\section{Classification}

\subsection{Supervised Classification}

\subsection{Unsupervised Classification}

\section{Text Representation and Word Vectors}

Word Vectors \cite{word2vec} are currently the state of the method of representing text as features for supervised and unsupervised classification problems. However, the exact benefits of Word2Vec are unclear without going over a brief history of simpler means of representation and textual feature extraction.

\subsection{Bag of Words}

\par{
The traditional method of text representation was a \textit{Bag of Words}. Bag of Words are unordered, sparse matrices where each column represents a unique term. As the English vocabulary $V$ is large and constantly changing, the first step of many Natural Language Processing tasks would be to scan over the corpus that you want to work with and build a list of each unique term. $V'$ is often smaller than $V$, but typos, proper names and slang all expand $V'$. For infinite datasets such as the Internet, one could utilize the \textit{Hashing Trick} \cite{} which runs each term into a hashing function that computes a random column index. This strategy has a few drawbacks such as colliding terms and having a $|V'|$ that is at least twice as large as your datasets predicted size, but no longer requires an additional pre-processing step.
}

\par{
Some classifiers, such as Naive Bayes \cite{}, work with these very wide and sparse matrices very well. For classifiers that prefer small, dense matrices there are several strategies such as Latent Dirichlet Allocation \cite{} and Singular Value Decomposition \cite{} that attempt to extract the most frequent and meaningful co-occurring terms and represent them as a single value. In addition, additional information can be injected by extracting \textit{Parts of Speech} -- such as \textit{Noun} or \textit{Adjective} -- or constructing the \textit{lemma} of terms -- swimming $\rightarrow$ swim. 
}


\subsection{AutoEncoders}

\par{
AutoEncoders were originally designed as compression strategies for video and text and were a source of inspiration for WordVectors due to its ability to create graphical features in an unsupervised manner. Graphical data tends to be very high fidelity, and so transferring it over the Internet losslessly is expensive. In addition, small errors and loss of quality is not a significant issue for videos that display dozens of frames per second.
}

\par{
Consider an grayscale image matrix $I$, that is \textsf{1000x1000} pixels. To simplify the problem, instead of a traditional RGB channel, each pixel is a float value in the range of $0..1$ indicating the grayscale of that pixel \footnote{which can be trivially converted to the RGB colorspace at render time}. Thus, the image contains 1 million floats if we were to represent it as a naive, dense matrix. Suppose our goal is to achieve 50\% compression. We would want to create two functions: $f_c$ and $f_d$. $f_c$ accepts $I$ and outputs a compressed version $C$ which is sent to the client, which $f_d$, then decodes and outputs $I'$. Our algorithm is trained to minimize the \textit{loss} which might be defined as the difference between $I$ and $I'$. Our black-box AutoEncoder would thus look something like this:
}

\[f_c(I) => C \]
\[ f_d(C) => I' \]
\captionof{figure}{S.t. $(I' - I)^2$ is minimized}

\par{
Intuitively, there is a trade-off in $|C|$ and the loss. That is, smaller, more-compressed matrices will generally result in a higher loss $I'$. In a typical system, AutoEncoders are \textit{stacked}. That is, we might have three successive compression functions, each emitting a compressed matrix followed by three successive decompression function.  The exact implementations of the AutoEncoder is beyond the scope of this paper, but they generally rely on a Deep Neural Network \cite{}.
}

\subsection{Word2Vec}

\par{
Mikolov's Word Vector model is one of the greatest recent developments in text classification \cite{word2vec}. Word Vectors attempt to improve upon the Bag of Words models in a few ways. First, the Bag of Words model treats every word as equally distant from each other. This is semantically incorrect. Words pairs such as \textit{lake} and \textit{swim} or \textit{queen} and \textit{king} are intuitively related compared to \textit{evil} and \textit{throttle}. This naturally leads into the second point: constructing a vectors with a limited number of dimensions such that similar words are closer than unrelated words. Third, suppose we could consume a corpus of billions of documents containing millions of unique terms and pre-compute our dense vectors. When we want to work with a new dataset, we no longer have to build our vocabulary and instead rely on our pre-computed vocabulary. Fourth, the system understands 
}

\par{
Mikolov's initial paper described two methods: \textsf{skip-gram} and \textsf{continuous bag of words (cbow)}. \textsf{skip-gram} focuses on predicting a word's \textit{context} from the word itself, whereas \textsf{cbow} focuses on predicting a \textit{word} from its context. 
}

\par{
Consider $w_t$ which is defined as a word $w$ at location $t$. $w_t$'s context $C_t$ is defined all words preceding or succeeding $w_t$ within a window of some parameter $n$. Now, we define a pair of functions $f$ and $g$ that are analogous to $f_c$ and $f_d$ in an AutoEncoder. $f$ accepts a sparse 1D vector where each column corresponds to a term in the vocabulary, just as it is in the Bag of Words model. $f$ will output a dense vector $V$ analogous to the compressed image $C$ in an AutoEncoder. In \textsf{skip-gram}, $f$'s input is $w_t$ and $g$'s output is $C_t'$, where the \textit{loss} is the difference between the predicted $C_t'$ and the actual $C_t$. In \textit{cbow}, $f$'s input is $C_t$ and $g$'s output is $w_t'$, where the \textit{loss} is the difference between the predicted $w_t'$ and the actual $w_t$. 
}


\begin{figure}
\[ C_t: [w_{t-N}, ..., w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}, ..., w_{t+N}] \]
\caption{A Context $C_t$ is the Bag of Words representation of the \textit{window} centered at point $t$, with $w_t$ omitted}
\end{figure}


\begin{figure}
\[ f(w_t) => V \]
\[ g(V) => C_t' \]
\caption{Skip-Gram: construct $f$ and $g$ such a word $w$ at location $t$ predicts its \textit{context} $C_t'$. The loss is the difference of $C_t'$ and its actual context $C_t$}
\end{figure}

\begin{figure}
\[ f(C_t) => V \]
\[ g(V) => w_t' \]
\caption{CBOW: construct $f$ and $g$ such a context $C$ at location $t$ predicts \textit{word} $w_t'$. The loss is the difference of the predicted $w_t'$ and the actual word $w_t$}
\end{figure}

\par{
By itself, the \textit{skip-gram} and \textit{cbow} models do not seem to be useful. The key contribution of Word Vectors is understanding that unlike in an AutoEncoder where a human is consuming the final output $C'$, \textit{we can throw away $g$ and its output entirely!}. Suppose we want to solve a second classification problem that has labels $L_t$. We can replace the original $g$ with a new $g'$ that accepts $V$ and predicts $L_t$! Thus, if we pre-compute a comprehensive look-up dictionary of words to $V$, we are able to greatly simplify the original dataset construction of multiple problems at the same time. 
}

\paragraph{Med2Vec} Building on top of Mikolov's Skip-Gram model, Choi et al. sought to create a deep learning document embedding strategy \cite{med2vec}. They had two datasets of 3 and 5 million documents with a combined total almost 30,000 medical codes that acted as a natural clustering. \textsf{Med2Vec} is constructed in a similar method as \textsf{Skip-Gram Word2Vec}, but treats sequential visits from the same patient as if they were sequence of words in a sentence. Compared to \textsf{Skip-Gram Word2Vec} and \textsf{GloVe}, \textsf{Med2Vec} achieves lower, better normalized Mutual Information Gain scores on Medication and Procedure, implying that it builds embeddings that are better clustered for those tasks \cite{GloVe}. While \textsf{SVD} of document text performed better than \textsf{Med2Vec}, \textsf{SVD} was demonstrated to have far lower interpretability \cite{svd}. 


\section{Existing Medical Language Methodologies}

We limit discussion of existing methologodies to two categories:
work on analysis of clinical records in the medical domain, primarily concentrating
here on NLP approaches; and emerging machine learning and NLP technologies.
In addition to these, our work on this project used a wide array of 
classification \cite{wu08} and association rule mining \cite{fpgrowth} techniques, and
traditional methods for text parsing and Parts-of-Speech (POS) tagging \cite{stanfordparser}. 
We used the Python \textsf{scikit-learn} \cite{scikit-learn} 
and \textsf{nltk} \cite{nltk} toolkits, the Stanford parser and Part of Speech Tagger \cite{stanfordparser}, and 
the Snowball stemmer for English\cite{snowball}. 

%% SKYLAR: EXPAND?
\paragraph{SentEmotion} 
The SentiMetrix team (consisting of the co-authors of this paper) became interested
in this challenge due to prior work SentiMetrix has conducted in the
domain of clinical diagnoses for mental health conditions \cite{coptads,coptads-book}.
Founded in 2007, SentiMetrix is a data science and social analytics company
offering a variety of solutions to address the “big data” needs of commercial companies and government agencies. 

Through its work on past projects \cite{dickerson14,kagan15,darpa} SentiMetrix has built an array of technology-based solutions, focusing on the near real-time analysis of large quantities of complex data in multiple languages. Prior projects
applied these solutions to data analytical challenges in the areas of national security, elections, marketing, and medicine.
We built upon our prior work on detecting mental health disorders such as Depression, PTSD and TBI from patient notes\cite{coptads,coptads-book}. This system, known as COPTADs, is a text-based classification engine that was developed in cooperation with psychologists. Leading surveys had identified a set of signals that a victim of depression might have: for example, isloation from others. We previously built a classification engine on top of the Stanford Parser \cite{stanfordparser} to extract these signals, 
and then a second layer to extract emotions such as anger or fear. The third layer generates a confidence value for each of the disorders that COPTADs is configured to recognize.

%% TODO: cite apriori?
\paragraph{Feature Selection using Association Rules} K. Rajeswari demonstrated the use of using \textsf{Apriori} rule mining in order to select features with high significance \cite{rajeswari}. First, the authors mined a set of rules with a small $k$ value. The second step was removing all features that did not appear in at least one rule. The third step was to re-run the \textsf{Apriori} but on a much larger $k$. Finally, when training the final classifier, they included only the features that were an antecedent in at least one rule. The higher $k$ value omits some useful association rules, but should take an order of magnitude less time to complete, due to the large reduction of $n$ candidate rules. The author noticed that on a dataset attempting to classify the risk of heart disease, that computation time was cut by a full two orders of magnitude with this procedure.

\paragraph{Classification using Association Rules} B. Lui developed a technique on which to do Classification Based On Association Rules \cite{cba}. Lui's main contributions is the notion of a \textit{coverage check} in which rules are sorted in order of their predictive power -- confidence -- and rules that are subsumed by a more powerful rule are removed. At inference time, the first rule in which the antecedent is covered yields its corresponding class. w. Li improves upon this work by raising the minimum coverage of the \textit{coverage check} to 5 subsuming rules, as well as a stronger notion of rules using chi-$x^2$ tests \cite{cmar}. 

%%% TODO: expand this some more
\paragraph{Other Work} Abbe \cite{abbe} describes different styles of psychiatric NLP and suggests four domains: observational studies, analysis of patient's thoughts and journals, medical records, and published literature. The NGRID challenge falls into the third category, whereas SentEmotion mostly focused on the second. Pestian \cite{pestian} created an NLP pipeline that could identify whether or not a suicide note was genuine using decision-tree-based classification rules along with AdaBoost and outperformed domain experts by reducing Type II errors by 30\%. Shiner \cite{shiner} processed 221 psychotherapy clinical notes to determine the quality of treatment that veterans were receiving. 

\section{Ensemble Construction} Constructing ensembles was a key step in winning both the N-GRID competition as well as others, such as Kaggle. However, constructing ensemble rules by hand can be time-consuming or miss optimal solutions. Cortes et al. describes an online machine-learning algorithm called \textsf{ESPBoost} that accepts hundreds of potential ``experts" and the correct label \cite{espboost}. \textsf{ESPBoost} uses coordinate descent to reduce the Hamming loss which finds a locally performant ensemble without enumerating all possibilities \cite{coordinate-descent,hamming-loss}. \textsf{ESPBoost} did best on large problems with a large number of experts.
 