\chapter{Related Work}\label{sec:related}

Machine Learning another name for the task known as \textit{predictive modeling}: given a set of observations in the past, can you construct a procedure that can identify future observations correctly? There are several different types of classification problems, but the two that we will be focusing on are supervised and unsupervised classification. 

\section{Classification}

Classification is the act of associating an \textit{observation} with a pattern \cite{liu2007web}. Formally, an observation -- typically represented as a collection of text, audiovisual or numerical \textit{features} -- is assigned one or more labels. When presented with a new, unlabeled observation, the \textit{classifier} must \textit{infer} or \textit{predict} the most likely label to associate with the observation. For example, if a data scientist were tasked with classifying days as \textit{hot} or \textit{cold}, the number of customers at a local ice cream store could be used as a feature. In general, we'd expect the store to be more busy on hot days, rather than cold days. However, this is not a guarantee -- for example, a particularly large birthday party could form an outlier, or perhaps ice cream stores are just more popular on Fridays, even though Fridays are no more likely to be warmer than any other day.

An excellent classifier would be able to identify hot days from cold ones with high \textit{precision} as well as miss very few obviously hot days (known as \textit{recall}). In most tasks, data scientists must balance between both Recall and Precision. In some domains, recall is extremely important. For example, in the medical domain it is much preferred to have false positives while detecting cancer than it is to overlook a real threat at the risk of a patient's health.  On the other hand, some tasks prefer the opposite. A camera that detects if someone has run a red light would prefer to be precise rather than flag innocent drivers.

%% 
\subsection{Supervised Classification}

Supervised Classification is focused on identifying membership to one of a set of \textit{predetermined} patterns, typically called a \textit{label} \cite{liu2007web}. Supervised learning is called supervised because it is assigned a label by a human that is decided upon before the algorithm is trained. The \textit{training} step will be given each label, and the label must be trustworthy and consistent. Supervised Classification problems, therefore, tend to be somewhat expensive to collect data for. Every data point must be labeled and audited by a human, or even several in order to form a clear consensus.

In the following paragraphs, a brief description will be provided of a variety of the most common supervised machine learning techniques that we used as a part of \CREATE. 

\paragraph{Naive Bayes}

Naive Bayes is one of the simplest machine learning algorithms \cite{liu2007web}. During training, the classifier assumes that each feature is independent and marks two pieces of information: the distribution of labels in the training set, and the distribution of values for which a certain value of a certain feature is associated with label. During inference, it computes the probability of a new observation with its previously recorded values and predicts the most likely class.

\paragraph{Decision Trees}

Decision Trees are classifiers modeled after a flowchart. The final result of the classifier is a simple of yes/no questions, terminating with a final label at each of its terminal nodes -- called \textit{leaves}. Since computing the best possible decision tree is difficult (NP-complete), usually a set of heuristics are employed \cite{liu2007web}.

To construct the classifier, a recursive algorithm is employed. The base case is when the input computes only a single class, or if every single attribute has already been analyzed. In that case, the result is a leaf node of the majority class. Otherwise, each remaining attribute \footnote{for very large numbers of attributes, someones only a random sample is analyzed at each level} is analyzed to see which attribute splits the dataset into two ``pure'' disjoint sets best. The selected attribute is removed and phrased as a ``question'' in the final classifier, while each of the disjoint sets are re-evaluated recursively until each sub-problem is terminated with only leaf nodes.

\begin{algorithm}[ht]
\SetKwData{DecisionTree}{DecisionTree}
\SetKwData{ImpurityEvalA}{ImpurityEval1}
\SetKwData{ImpurityEvalB}{ImpurityEval2}
\SetKwData{ArgMax}{argmax}

\SetKwInOut{D}{D}
\SetKwInOut{A}{A}
\SetKwInOut{T}{T}

\D{True Labels}
\A{Features}
\T{Tree to recursively build (initially empty)}

\If{\textit{D} contains only one class}{
    \textit{make T a leaf node labeled with the majority class} \;
}
\ElseIf{\texit{A} is empty}{
    \textit{make T a leaf node labeled with the majority class} \;
}
\Else{
    \emph{(sometimes, only a random sample of attributes are tested)} \\

    $p_0 \gets $ \ImpurityEvalA{$D$} \;
    \For{$A_i$ in $A$}{
        $p_i \gets $ \ImpurityEvalB{$A_i, D$} \;
    }
    $g \gets $ \ArgMax{$p_1 \dots p_k$} \;
    \If{$p_0 - p_g < threshold$}{
        \textit{make T a leaf node labeled with the majority class}
    }
    \Else{
        $T_j \gets $ \textit{make T a decision node on $A_g$} \;
        \textit{partition \textit{D} into disjoint subsets for each value of $A_g$} \;
        \For{$D_j$ in partitions}{
            \DecisionTree{$D_j, A - A_g, T_j$} \;
        }
    }
}

\caption{Decision Tree construction \cite{liu2007web}}
\end{algorithm}

\paragraph{AdaBoost} \label{sec:adaboost}

AdaBoost is a methodology in which an \textit{ensemble} of weak learners are trained in succession; each learner specializing in correcting the errors the previous learner made. AdaBoost traditionally uses fast and weak learners such as Naive Boost or Decision Trees, as the initial weak learners only have to perform slightly better than random in order to eventually converge to a stronger classifier \cite{liu2007web}.

First, a base classifier is trained. Then, there is a \textit{boosting} step: this step computes all the training instances that were correctly or incorrectly identified. Those which were correctly identified are assigned a weaker level of importance for the successive classifier, while incorrectly classified observations are given a higher level of importance. Finally, the next classifier is trained with the re-weighted dataset \cite{adaboost}. 

The last two steps are repeated a predetermined number of times, or until a learner cannot improve on a previous learner. 

\paragraph{Random Forest}

Random Forests is an ensemble of Decision Trees that are trained on random subsamples of the training data \cite{breiman01}. Decision Trees suffer from a few disadvantages. First, for many problems decision trees tend to become very long and complex. This results in the tree overfitting the training data, and building patterns out of random noise; when you classify against out of sample data, it does not generalize very well.

However, you can generally eliminate this overfit without reducing performance by transforming the Decision Tree into a Random Forest. First, you must determine how many subtrees you would like to train.
Then, randomly subsample (with replacement) the data into that many subsets. Finally, construct a Decision Tree with each of those subsamples, with one small change: at each level of the Decision Tree subroutine, only sample a random subset of the features, rather than the entire feature set. This is to provide some entropy for the subtrees so that a handful of dominating features force almost every subtree to look the same. At inference, take the average of all the votes.

\begin{algorithm}
\SetKwData{DecisionTree}{DecisionTree}
\SetKwData{SubSample}{SubSample}

\SetKwInOut{D}{D}
\SetKwInOut{A}{A}
\SetKwInOut{B}{B}

\D{True Labels}
\A{Features}
\B{Number of subtrees to construct}

\For{$i$ in 0\dots$B$}{
    $D_i, A_i \gets $ \SubSample{i, D, A} \;
    $T_i \gets null$ \;
    \emph{only sample square-root features in each level of DT} \\
    \DecisionTree{$D_i$, $A_i$, $T_i$, \sqrt{A}} ; \\
    $Trees_i \gets T_i$ \;
}

\caption{Random Forest Construction \cite{scikit-learn}}
\end{algorithm}

\paragraph{Support Vector Machines}

Support Vector Machines attempt to identify how to bisect the feature space into two classes. Linear SVMs always bisect with a line in the form of $Ax + b = y$; though there are many \textit{kernels} that can be used to transform the feature space to solve more interesting problems. The observations closest to the bisecting plane are called \textit{support vectors}. In higher dimension problems, the composition of these vectors constructs a bisecting hyperplane. During inference, the label that is returned depends on whether the point is above or below the hyperplane.

\subsection{Unsupervised Classification}

Unsupervised Classification does not have labels, and instead attempts to assign certain patterns to groups, typically called \textit{clusters} \cite{celebi2015partitional}. Some algorithms expect hints in the form of the exact number or size of clusters, while others attempt to figure out it out on their own. Unsupervised classification is often used to provide insight for humans, especially for topic modeling \cite{celebi2015partitional}. Unsupervised classification can also be used as an automated way to learn compression and feature extraction methodologies, which is what we will focus on in the section \ref{sec:wordvectors}.  

\section{Text Representation and Word Vectors} \label{sec:wordvectors}

WordVectors \cite{word2vec} are currently the state-of-the-art method of representing text as features for supervised and unsupervised classification problems. However, the exact benefits of WordVectors are unclear without going over a brief history of simpler means of representation and textual feature extraction.

\subsection{Bag of Words}

\par{
The traditional method of text representation was a \textit{Bag of Words}. Bag of Words are unordered, sparse matrices where each column represents a unique term. As the English vocabulary $V$ is large and constantly changing, the first step of many Natural Language Processing tasks would be to scan over the corpus that you want to work with and build a list of each unique term. $V'$ is often smaller than $V$, but typos, proper names and slang all expand $V'$. For infinite datasets such as the Internet, one could utilize the \textit{Hashing Trick} \cite{weinberger2009feature} which runs each term into a hashing function that computes a random column index. This strategy has a few drawbacks such as colliding terms and having a $|V'|$ that is at least twice as large as your datasets predicted size, but no longer requires an additional pre-processing step.
}

%% Add citation for Singular Value Decomposition
\par{
Some classifiers, such as Naive Bayes \cite{rish2001empirical}, work with these very wide and sparse matrices very well. For classifiers that prefer small, dense matrices there are several strategies such as latent Dirichlet Allocation \cite{blei2003latent} and Singular Value Decomposition \cite{liu2007webi} that attempt to extract the most frequent and meaningful co-occurring terms and represent them as a single value. Moreover, additional information can be injected by extracting \textit{Parts of Speech} -- such as \textit{Noun} or \textit{Adjective} -- or constructing the \textit{lemma} of terms -- swimming $\rightarrow$ swim. 
}

\subsection{AutoEncoders}

\par{
AutoEncoders were originally designed as compression strategies for video and text and were a source of inspiration for WordVectors due to its ability to create graphical features in an unsupervised manner. Graphical data tends to be very high fidelity, and so transferring it over the Internet losslessly is expensive. In addition, small errors and loss of quality are not significant issues for videos that display dozens of frames per second.
}
%% Should it be "0...1" or are two periods standard?
\par{
Consider an grayscale image matrix $I$, that is \textsf{1000x1000} pixels. To simplify the problem, instead of a traditional RGB channel, each pixel is a float value in the range of 0\ldots1 indicating the grayscale of that pixel. Thus, the image contains 1 million floats if we were to represent it as a naive, dense matrix. Suppose our goal is to achieve 50\% compression. We would want to create two functions: $f_c$ and $f_d$. $f_c$ accepts $I$ and outputs a compressed version $C$ which is sent to the client, which $f_d$, then decodes and outputs $I'$. Our algorithm is trained to minimize the \textit{loss} which might be defined as the difference between $I$ and $I'$. Our black-box AutoEncoder would thus look something like this:
}

\begin{figure}[h]
\[f_c(I) => C \]
\[ f_d(C) => I' \]
\caption{S.t. $(I' - I)^2$ is minimized}
\end{figure}

%% Caption for AutoEncoder
\par{
Intuitively, there is a trade-off in $|C|$ and the loss. That is, smaller, more-compressed matrices will generally result in a higher loss $I'$. In a typical system, AutoEncoders are \textit{stacked}. That is, we might have three successive compression functions, each emitting a compressed matrix, followed by three successive decompression functions. The exact implementations of the AutoEncoder is beyond the scope of this discussion, but they generally rely on a Deep Neural Network \cite{tensorflow}.
}

\subsection{Word2Vec}

\par{
Mikolov's Word Vector model is one of the greatest recent developments in text classification \cite{word2vec}. Mikolov identified a few weaknesses of the Bag of Words model. First, constructing Bag of Word features requires two passes over the data \footnote{at least, not without using the Hashing Trick, which has its own drawbacks as discussed earlier}, which can be very costly when working with billions of documents. Second, if one could construct a model that captures all English literature with reasonable precision, then that model can be trained once and then used on nearly every English NLP problem. However, a "Universal English" Bag of Words model would be enormous and contain many unused words for most practical applications. The final and most significant flaw is how Bag of Words treats every word as equally distant from each other. This is intuitively imprecise. Words pairs such as \textit{\{\textit{lake}, \textit{swim}\}} or \textit{\{\textit{queen}, \textit{king}\}} are intuitively related compared to \textit{\{\textit{evil}, \textit{throttle}\}}.
}

%% Why aren't these paragraphs indented?
\par{
Word vectors solves this by constructing dense, continuous vectors with a limited number of dimensions such that similar words are closer than unrelated words. If vector space representations of \textit{queen} and \textit{king} were close, then we would have at least a partial success at solving the third problem. If our training model is capable of an online training that can handle billions of documents in a reasonable amuont of time, this would go a long way towards addressing the first two flaws.
}

\par{
Mikolov's initial paper described two methods: \textsf{skip-gram} and \textsf{continuous bag of words (cbow)}. \textsf{skip-gram} focuses on predicting a word's \textit{context} from the word itself, whereas \textsf{cbow} focuses on predicting a \textit{word} from its context. 
}

\par{
Consider $w_t$ which is defined as a word $w$ at location $t$. $w_t$'s context $C_t$ is defined by all words preceding or succeeding $w_t$ within a window of some parameter $n$. Now, we define a pair of functions $f$ and $g$ that are analogous to $f_c$ and $f_d$ in an AutoEncoder. $f$ accepts a sparse 1D vector where each column corresponds to a term in the vocabulary, just as it is in the Bag of Words model. $f$ will output a dense vector $V$ analogous to the compressed image $C$ in an AutoEncoder. In \textsf{skip-gram}, $f$'s input is $w_t$ and $g$'s output is $C_t'$, where the \textit{loss} is the difference between the predicted $C_t'$ and the actual $C_t$. In \textit{cbow}, $f$'s input is $C_t$ and $g$'s output is $w_t'$, where the \textit{loss} is the difference between the predicted $w_t'$ and the actual $w_t$. 
}


\begin{figure}[H]
\[ C_t: [w_{t-N}, \ldots, w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}, \ldots, w_{t+N}] \]
\caption{A Context $C_t$ is the Bag of Words representation of the \textit{window} centered at point $t$, with $w_t$ omitted}
\end{figure}


\begin{figure}[H]
\[ f(w_t) => V \]
\[ g(V) => C_t' \]
\caption{Skip-Gram: construct $f$ and $g$ such a word $w$ at location $t$ predicts its \textit{context} $C_t'$. The loss is the difference of $C_t'$ and its actual context $C_t$}
\end{figure}

\begin{figure}[H]
\[ f(C_t) => V \]
\[ g(V) => w_t' \]
\caption{CBOW: construct $f$ and $g$ such a context $C$ at location $t$ predicts \textit{word} $w_t'$. The loss is the difference of the predicted $w_t'$ and the actual word $w_t$}
\end{figure}

\par{
By itself, the \textit{skip-gram} and \textit{cbow} models do not seem to be useful. The key contribution of Word Vectors is understanding that unlike in an AutoEncoder where a human is consuming the final output $C'$, \textit{we can throw away $g$ and its output entirely!} Suppose we want to solve a second classification problem that has labels $L_t$. We can replace the original $g$ with a new $g'$ that accepts $V$ and predicts $L_t$! Thus, if we pre-compute a comprehensive look-up dictionary of words to $V$, we are able to greatly simplify the original dataset construction of multiple problems at the same time. 
}

\paragraph{Med2Vec} \label{sec:med2vec} Building on top of Mikolov's Skip-Gram model, Choi et al. sought to create a deep learning document embedding strategy \cite{med2vec}. They had two datasets of 3 and 5 million documents with a combined total almost 30,000 medical codes that acted as a natural clustering. \textsf{Med2Vec} is constructed in a similar method as \textsf{Skip-Gram Word2Vec}, but treats sequential visits from the same patient as if they were sequence of words in a sentence. Compared to \textsf{Skip-Gram Word2Vec} and \textsf{GloVe}, \textsf{Med2Vec} achieves lower, better normalized Mutual Information Gain scores on Medication and Procedure, implying that it builds embeddings that are better clustered for those tasks \cite{GloVe}. While \textsf{SVD} of document text performed better than \textsf{Med2Vec}, \textsf{SVD} was demonstrated to have far lower interpretability \cite{svd}. 

\section{Existing Medical Language Methodologies}

We limit discussion of existing methologodies to two categories:
work on analysis of clinical records in the medical domain, and emerging machine learning and NLP technologies.
In addition to these, our work on this project used a wide array of 
classification \cite{wu08} and association rule mining \cite{fpgrowth} techniques, and
traditional methods for text parsing and Parts-of-Speech (POS) tagging \cite{stanfordparser}. 
We used the Python \textsf{scikit-learn} \cite{scikit-learn} 
and \textsf{nltk} \cite{nltk} toolkits, the Stanford parser and Part of Speech Tagger \cite{stanfordparser}, and 
the Snowball stemmer for English\cite{snowball}. 

%% SKYLAR: EXPAND?
\paragraph{SentEmotion} 
Through its work on past projects \cite{dickerson14,kagan15,darpa} SentiMetrix has built an array of technology-based solutions, focusing on the near real-time analysis of large quantities of complex data in multiple languages. SentEmotion \cite{coptads-book,coptads} is a text-based classification engine developed jointly with psychologists that detects mental health disorders such as \textit{Depression}, \textit{Post-Traumatic Stress Disorder}, and \textit{Traumatic Brain Injury} from patient notes. Leading surveys had identified a set of signals that a victim of depression might have: for example, isolation from others. We built a classification engine on top of the Stanford Parser \cite{stanfordparser} to extract these signals, and then a second layer to extract both emotions -- anger or fear -- and symptoms -- insomnia or agoraphobia. The final layer generates a confidence value for each of the disorders that COPTADs is configured to recognize. 


\paragraph{cTAKES} Clinical Text Analysis and Knowledge Extraction System \cite{ctakes} by Apache is an open-source NLP system that extracts a variety of mental illness signals, physical symptoms, and medication from text.

%% TODO: cite apriori?
\paragraph{Classification using Association Rules} B. Liu developed a technique on which to do Classification Based On Association Rules \cite{cba}. Liu's main contributions is the notion of a \textit{coverage check} in which rules are sorted in order of their predictive power -- \textit{confidence} -- and then removes rules that are subsumed by a more powerful rule. At inference time, the first rule in which the antecedent is covered yields its corresponding class. Li et all improves upon this work by raising the minimum coverage of the \textit{coverage check} to 5 subsuming rules, as well as a stronger rule scoring using chi-$x^2$ tests \cite{cmar}. In addition, Li suggests storing rules in a trie so that inference can be performed quickly. 

\paragraph{Feature Selection using Association Rules} K. Rajeswari continued Liu's work to see if \textsf{Apriori} rule mining can be used select features with high significance, specifically in order to classify the risk of heart disease \cite{rajeswari}. First, the authors mined a set of rules with a small $k$ value. Then, they removed all features that did not appear in at least one rule. Next, they re-ran the \textsf{Apriori} but on a much larger $k$. Finally, when training the final classifier, they included only the features that were an antecedent in at least one rule. The higher $k$ value omits some useful association rules, but should take an order of magnitude less time to complete, due to the large reduction of $n$ candidate rules. The author conclude that this process reduces computation time on their dataset by two full orders of magnitude.

%%% TODO: expand this some more
\paragraph{Other Work} Abbe \cite{abbe} describes different styles of psychiatric NLP and suggests four domains: observational studies, analysis of patient's thoughts and journals, medical records, and published literature. The NGRID challenge falls into the third category, whereas SentEmotion mostly focused on the second. Pestian \cite{pestian} created an NLP pipeline that could identify whether or not a suicide note was genuine using decision-tree-based classification rules along with AdaBoost and outperformed domain experts by reducing Type II errors by 30\%.

\section{Ensemble Construction} Constructing ensembles was a key step in winning both the N-GRID competition as well as others, such as Kaggle. However, constructing ensemble rules by hand can be time-consuming or miss optimal solutions. Cortes et al. describes an online machine-learning algorithm called \textsf{ESPBoost} that accepts hundreds of potential ``experts" and the correct label \cite{espboost}. Similar to many other machine learning algorithms, \textsf{ESPBoost} uses coordinate descent to reduce a loss function -- in this case, Hamming -- to find a local minima without enumerating all possibilities \cite{coordinate-descent,hamming-loss}. \textsf{ESPBoost} has been empirically found to work best on large problems with a large number of experts.
 