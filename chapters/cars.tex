\chapter{Class Association Rules}\label{sec:assoc-rules} 
 
 We decided to see if we could discover
 some clear dependencies between the features present in
 (potentially small) subsets
 of patients, and the value of their \textsf{Valence}. To test this, we engaged in the mining of our feature data 
 for Class Association Rules.
 
 \section{Data Preparation}
 
 We constructed a subset of binary and categorical features found in the data.
 These primarily included the original features, medication and cumulative
 features along with boolean features from LIWC, SentEmotion, cTakes.
 With these, we concentrated on discovery of class association rules
 of the form:
 
 \begin{figure}[H]
 $$ F_1,F_2,\ldots, F_k\longrightarrow \mathsf{Valence},$$
 \caption{Antecedent and Consequent of a Class Association Rule}
 \end{figure}

\noindent where $F_1,\ldots, F_k$ are conditions on the binary/categorical features. Table \ref{tab:AR}
shows the parameters for our Class Association Rule search; we pruned away all rules that
did not satisfy them.

\section{Mining}

We used an existing Python implementation \cite{python-fp-growth} of the \textsf{FP-Growth} \footnote{
Since the original Python implementation is not actively maintained, SentiMetrix has a private fork of the repository.
SentiMetrix's API tweaks allow the emission of only Class Association Rules, rather than all Association Rules;
 support for aggressive filtering of redundant rules while mining; and utilizes Numpy arrays rather than Python lists for more compact memory allocation and faster cache coherence \cite{numpy}. The overall improvements result in a modest reduction of memory, and a 33\% reduction in run-time. In addition, considering only Class Association Rules reduces the problem size by multiple orders of magnitude. This is significant, because even with these improvements mining higher $k \in \{4, 5\}$ took days to complete. 
 } \cite{fpgrowth} 
algorithm to perform an exhaustive search for Class Association Rules with
$k \in \{1,2,3,4,5\}$.  For larger values of $k$ ($k = 6\ldots 9$) we used
$C5$ \cite{c5,c45}, which is non-exhaustive.

\begin{table}
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \textsf{Parameter} & \textsf{Value} \\
    \hline
    Minimal Support  &  20 records\\ 
    \hline
    Minimal Confidence & 0.6 \\
    Maximal Inverse Confidence & 0.4\\
    Maximal Negative Confidence& 0.4\\
    \hline
    \end{tabular}
    \caption{Pruning conditions for Association Rule mining process.}
    \label{tab:AR}
\end{table} 
 
  The discovered rules went through a rigorous pruning procedure. In addition
  to pruning away all discovered Class Association Rules (CARs) that did not
  pass the minimum standards shown in Table \ref{tab:AR}, we also conducted
  a $\chi^2$ test of significance for each discovered CAR (see Section \ref{sec:selection}
  for a more detailed explanation of the $\chi^2$ tests conducted). All CARs
  that did not pass the $\chi^2$ test at the significance level of
  $p=0.05$ were also eliminated from consideration. Failing
  the $\chi^2$ test implies that
  the CAR was a by-product of individual frequencies of the features it contained,
  rather than an actual meaningful relationship between these features and
  the \textsf{Valence} variable.
  
  \section{Pruning}
  
Finally, we performed a \textit{Coverage Test} as proposed by Li et al \cite{cmar}. The purpose of the \textsf{Coverage Test} is to reduce the set of CARs to the ones that most accurately describe our data while avoiding excessive duplication. First, we sorted all of our generated CARs by confidence, support and $\chi^2$ score from best to worst. Starting with the first rule, all documents with features in the antecedent of the rule were marked. Then, we advanced onto the second rule and marked all documents with features in the antecedent of that rule. The process was repeated until each input record was covered. After a single document has been marked five times, we removed it from future consideration. If a rule did not \textit{cover} any considered documents, we discarded the rule. Once all documents have been marked five times, we discarded all remaining rules. For more information, see Algorithm \ref{alg:coveragecheck}.
 
\begin{algorithm}[h]
\SetKwData{Covers}{covers}
\SetKwData{Rank}{Rank}
\SetKwData{Zip}{Zip}
\SetKwData{Array}{Array}
\SetKwInOut{rules}{rules}
\SetKwInOut{observations}{observations}
\SetKwInOut{Output}{output}
\SetKwInOut{k}{k}

\rules{Sorted set of candidate association rules (best to worst)}
\observations{Set of observations to cover}
\k{number of observations to cover an observation}
\Output{A set of covering association rules}
\BlankLine

$observation\_counts \gets$ \Array{\Rank{$observations$}} \;
\For{$rule$ in $rules$}{
    $keep \gets false$ \;
    \For{$observation, count$ in \Zip{$observations$, $observation\_counts$}}{
        \If{$count < k$ and \Covers{$rule, observation$}} {
            $keep \gets true$\;
            \emph{increment the corresponding $observation\_count$} \;
        }
    }
    \If{$keep$}{
        \emph{add our current $rule$ to $output$} \;
    }
}
\caption{A naive CAR coverage check algorithm as described in \cite{cmar}} \label{alg:coveragecheck}
\end{algorithm}
 
 
 Altogether, the pruning process reduced the total number of CARs extracted from the data from 345,373 to 628.
 \textit{For each extracted Class Association Rule, we added one binary feature to the dataset,
 which was set to 1 on records where the antecedent of the Association Rule applied} \footnote{The conclusion of the CAR was not considered, as that would result in leaking the label information during training, nor could these features be constructed on a hidden test set}. Some examples
 of the Association Rules we mined during this process are presented in Table \ref{tab:ARexamples}. The first two rules were found by the \textsf{FP-Growth}
 process, and the third by \textsf{C5}.
 
\begin{table}
     \centering
     \begin{tabular}{|l|l|c|c|c|}
    \hline
    \textsf{Antecedent}& \textsf{Valence} & \textsf{Support}& \textsf{Conf.} &
    \textsf{Neg. Conf.}\\
    \hline
    \small{\texttt{patient is an inpatient} and}  &    &     &   & \\
    \small{\texttt{currently undergoing}} & \textsf{SEVERE} & 22 & 21/22 & 18.25\%\\
    $\ \ \ \ \ $ \small{\texttt{ addiction treatment}} & & & (95.5\%) & \\
    \hline
    \small{\texttt{patient suffers from OCD} and}  &   & & & \\
    \small{\texttt{has no history of drug abuse}} & \textsf{MOD.} & 25 & 20/25 & 22.06\%\\
    \small{\texttt{and NOT taking Aplenzin}} & & & (80\%)  & \\
    \hline
    \small{\texttt{patient is NOT inpatient} and}  & & & & \\
    \small{\texttt{does not drink alcohol} and}  & & & & \\
    \small{\texttt{is NOT taking Allernaze} and}  & & & & \\
    \small{\texttt{is NOT taking Levothroid} and}  & & & & \\
    \small{\texttt{is NOT taking Cultivate} and} & \textsf{MILD} & 73 & 67/73 & 38.37\%\\
    \small{\texttt{is NOT taking Abilify} and}  & & & (92\%) & \\
    \small{\texttt{does not suffer from OCD} and}  & & & & \\
    \small{\texttt{has no history of violence}} & & & & \\
    \small{\texttt{and suffers from depression}} & & & & \\
    \hline
     \end{tabular}
     \caption{Examples of discovered Association Rules.}
     \label{tab:ARexamples}
 \end{table}