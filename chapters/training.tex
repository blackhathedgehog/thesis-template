\chapter{Classifier Training and Adaptations}\label{sec:ml}

\begin{table}[t]
{\small
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textsf{No.}& \textsf{Abbreviation} & \textsf{Classifier} & \textsf{Source}\\
    \hline
    1 & \textsf{SVM-Ini}& \textsf{AdaBoost \cite{adaboost} \cite{scikit-learn}} & \textsf{SentiMetrix} \\
      & \textsf{AdaBoost} & \textsf{1 round Linear-SVM \cite{cortes95}},  49 rounds of NB  & \\
    2 & \textsf{RF-reg-clf} & \textsf{Train: Regression RF \cite{ho95}}; & \textsf{SentiMetrix} \\
      & & \textsf{inference: Classification RF} & \\
    3 &\textsf{MI SVR}&  \textsf{Mutual-Info \cite{ross14} Feature Boosted SVM \cite{cortes95}} & \textsf{SentiMetrix} \\
    4 & \textsf{CMAR} &  \textsf{Classifier on Multiple Assoc. Rules \cite{cmar}}& \textsf{SentiMetrix} \\
    5 & \textsf{CMAR SVM}& \textsf{CMAR-Boosted \cite{cmar} SVM \cite{cortes95}}& \textsf{SentiMetrix} \\
    6 & \textsf{CBA} & \textsf{Classification Based on Associations \cite{cba}}& \textsf{SentiMetrix} \\
    \hline
    7 & \textsf{MB NB} & \textsf{Multinomial/Bernoulli Na\"{i}ve Bayes \cite{scikit-learn}} & \textsf{scikit-learn} \\
    8 & \textsf{Lin-SVM}& \textsf{Linear Kernel SVM \cite{cortes95}} & \textsf{scikit-learn} \\
    9 & \textsf{RBF SVM}& \textsf{Radial Basis Function Kernel SVM \cite{scikit-learn}} & \textsf{scikit-learn} \\
    10 & \textsf{LogReg} & \textsf{Logistic Regression \cite{scikit-learn}} & \textsf{scikit-learn}\\
    11 & \textsf{RF} & \textsf{Random Forests \cite{ho95}} & \textsf{scikit-learn} \\
    12 &  \textsf{Adaboost NB}& \textsf{AdaBoosted NaiveBayes \cite{adaboost}}& \textsf{scikit-learn} \\
    13 &  \textsf{KNN} & \textsf{K-Nearest Neighbors \cite{scikit-learn}} & \textsf{scikit-learn} \\
    14 & \textsf{SGD} & \textsf{Stochastic Gradient Descent \cite{scikit-learn}}& \textsf{scikit-learn} \\
    15 & \textsf{BRBM} & \textsf{Bernoulli Restricted Boltzmann Machine \cite{scikit-learn}} & \textsf{scikit-learn}\\
    %followed by Random Forest/KNN/SVM/LogisticRegression} \\
    16 & \textsf{RF-reg} & \textsf{Regression version of Random Forest \cite{ho95}}& \textsf{scikit-learn}\\
    17 & \textsf{Lin-SVM-reg} & \textsf{Regression version of Lin-SVM \cite{cortes95}}& \textsf{scikit-learn}\\
    18 & \textsf{RBF SVM-reg} & \textsf{Regression version of RBF SVM \cite{cortes95}}& \textsf{scikit-learn}\\
    \hline
    19 & \textsf{DNN} & \textsf{Deep Neural Network \cite{tensorflow}}& \textsf{TensorFlow} \\
    20 & \textsf{Deep \& Wide} & \textsf{Deep-and-Wide classifier \cite{tensorflow}}& \textsf{TensorFlow} \\
    \hline
    21 & \textsf{XGBoost}& \textsf{XGBoost (scalable gradient boosting) \cite{xgboost}}& \textsf{XGBoost} \\
    22 & \textsf{C5} & \textsf{C5.0 Decision Tree Classifier \cite{c5}} & \textsf{RuleQuest} \\
    \hline
    \end{tabular}
    \caption{All the classifiers that were tried as part of the N-GRID Shared Task Challenge}
    \label{tab:Classifiers}
}
\end{table}

%% 2. discuss how we ran any of the classifiers (where this is not immediately clear

For our next step, we have constructed a battery of 22 different classifiers
to train on the dataset we built on previous steps. 
Table \ref{tab:Classifiers} lists the classifiers we
used on this project. 12 of the 22 classifiers came from \textsf{scikit-learn}.
Another five classifiers came from the internal SentiMetrix implementations
primarily developed prior to the N-GRID challenge, but modified where needed
to work with the data from this challenge.  Additionally, we used two 
neural network learners from Google's \textsf{TensorFlow}: their deep neural network
implementation; and their so called \textit{deep and wide} classifier, which
combines neural nets (deep learning) with Support Vector Machines (wide learning).
Finally, two extra classifiers --- \textsf{XGBoost}, the boosted gradient classifier
\cite{xgboost}, and Quinlan's implementation of
C5.0 decision tree classifier \footnote{Due to licensing restrictions, we did not integrate C5.0 into the Common Pipeline Framework} \cite{c5} --- were used as well.

Of the 22 classifiers we used two, the Random Forest Regression with Classification Inference (\textsf{RF-reg-clf} in Table \ref{tab:Classifiers}, and the SVM-initialized
Na\"{i}ve Bayes AdaBoost were novel adaptations of the well-known
Random Forest \cite{breiman01, ho95} and AdaBoost \cite{adaboost} machine learning
techniques. They are described below.

\section{Classifier Adaptations}\label{sec:newml}

Our two novel adaptations of existing classifiers are discussed below. 

\subsection{Random Forest Regression with Classification Inference (RF-reg-clf)}
Random Forest is a powerful yet forgiving algorithm that can perform a modest amount of feature selection due to its subsampling \cite{breiman01, ho95}. In \textsf{scikit-learn}, there are both regression and classification modes of Random Forest \cite{scikit-learn}. As Valence can be treated as both a class or an ordinal value, we tried both methods. Since regression provides additional insight for the classifier, it often had slightly higher scores. However, in practice regression at inference time biases the kernels right in between \textsf{MILD} (eg, 1.4) and \textsf{MODERATE} (eg, 1.6). The result of this is \textsf{MILD} Valences might be moved to be slightly more \textsf{MODERATE} and vice-versa. When it comes to building the ensemble, this small amount of drift can result in large classification errors if it causes the ensemble's vote to cross a rounding threshold. Our adaptation was to train the Random Forest on the regression version of the problem. Then, during inference, round the inferred value to the nearest Valence. 

\subsection{SVM Initialized AdaBoost (SVM-Init-ada)}
The novel technique we used on this project is the initialization of AdaBoost learning
process with SVM (\textsf{SVM-Init-AdaBoost} classifier in our parlance). AdaBoost trains
a sequence of  estimators one after another \cite{adaboost}. After each iteration, the training set is be reweighed; documents that were just misclassified will have their weight increased, while documents that were just classified correctly will have their weight decreased. This forces the next classifier to correct the mistakes its predecessor made. While AdaBoost is traditionally done a fast and weaker classifier such as Naive Bayes, any kernel can be used. 

In other project, SentiMetrix has had success by introducing a single round of a slow and strong classifier as a seed for AdaBoost \cite{darpa}. Recall that AdaBoost generally relies on an ensemble of weak classifiers that only need to be slightly better than random to gradually converge. As Support Vector Machines was one of our better classifiers and provides a meaningful decision function that cleanly divides the problem space, it works well as an intitial \textit{bootstrapping} classifier to provide an anchor for the successive weak classifier to converge around. We modified the original AdaBoost process (see Section \ref{sec:adaboost}) as follows:

\begin{enumerate}
    \item \textsf{Step 1:} Run Linear kernel SVM classifier on input data. 
    \item \textsf{Step 2:} Analyze kernel decision function to reweigh document weights
    \item \textsf{Step 3\ldots52:} Run Na\"{i}ve Bayes classification 49 times, reweighing document weights after every iteration, and checking for convergence. Reaching convergence before 49 iterations will terminate the process early
\end{enumerate}

On the input N-GRID challenge data, linear kernel SVM produced better accuracy results than a single Na\"{i}ve Bayes run. This allowed our modification of AdaBoost to start with a sufficiently accurate bootstrap. This additional accuracy gained on the first step has proven to be  a core factor in the overall accuracy of this classifier, as one of its runs wound up being the best individual classifier in our battery.

One might point out that if 1 iteration of SVM is good, then wouldn't 50 iterations of SVM to be even better? First, there is a significant cost to training and hyper-parameterizing a Support Vector Machine, compared to Naive Bayes -- hours versus seconds. Second, the Support Vector Machine is inserted for ``free'' by a natural consequence of our process. Since we had already trained and hyper-parameterized a Support Vector Machine for each view, we only had to train the 49 successive iterations of Naive Bayes in order to build this model. Finally, such a process would be similar to that of the Gradient Boosted Decision Trees implemented in XGBoost \cite{xgboost}, which did not perform very well in our data sets. This was partially expected as XGBoost typically requires millions of observations to generalize well.

\section{Data Views}

Each of or 22 classifiers 
was separately trained on nine different \textit{data views} described
in Table \ref{tab:dataviews}.  A data view is a collection of features
onto which the data is projected prior to being supplied to the classifier.
Different subsets of features were selected due to their distinct origins,
and the desire of our team to see if certain minimalistic sets of
features contain enough information for training the classifiers.

Two of the nine data views listed, 
\textsf{ANOVA-wordvector-34} (the 34 \textsf{Word2Vec} features that
passed our ANOVA significance tests) and \textsf{WordVector}
(all 300 \textsf{Word2Vec} features) yielded abysmal accuracy for
all classifiers, and were eliminated from further consideration.

\textit{Of the remaining seven data views} one, \textsf{Full}, represents 
the entire collection of features selected during
the process described in Section \ref{sec:selection}, five are its subsets,
and one, \textsf{TF-IDF} is the complete set of tf-idf vectors
representing the textual portion of each record. The subsets
of the \textsf{Full} data view were selected to represent different categories
of features (\textsf{CARs}, \textsf{Numeric}\footnote{The name
of this view is a bit of a misnomer, and is kept for historical reasons.
This view includes both numeric and categorical features that
were present in the original dataset, as well as constructed
using \textsf{cTAKES}, SentEmotion, and \textsf{LIWC} toolkits.}) as well
as the best features that passed a specific test: $\chi^2$, ANOVA or Multiple
Information Gain. We experimented briefly with top 100, 82 \footnote{82 was the fewest number of significant features found by all three statistical measures -- which happened to be $chi^2$} and top 75 best features
for each of the tests, but settled on top 50, as this provided better accuracy.

As a result, at the end of the process we had a total of $22\times 7 = 154$
trained (classifier,data view) pairs to go through. As a final preprocessing step, we normalized the data view as appropriate for each classifier. For most classifiers, the normalization centered each feature and scaled it to have unit standard deviation using the interquartile range. For classifiers that cannot use negative numbers, such as Multinomial NB, we did the above normalization and then rescaled the data in the range of $[0, 1]$. This was accomplished with \textsf{scikit-learn}'s \textsf{RobustScaler} and \textsf{MinMaxScaler}, respectively \cite{scikit-learn}. 


\begin{table}[]
    \centering
    \begin{tabular}{|l|l|l|c|}
    \hline
    \textsf{No.} & \textsf{Label} & \textsf{Explanation} & \textsf{Size}\\
    \hline
        1 & \textsf{Full} & All features that passed filtering & 788 \\ 
        2 & \textsf{Numeric} & All numeric (and categorical) filtered & 125\\ 
        3 & \textsf{CARs} & All CAR features & 628 \\
        4 & \textsf{TF-IDF} & All tf-idf unigram features & 8033\\
        5 & \textsf{WordVector}& All \textsf{Word2Vec} features & 300\\
        6 & \textsf{Chi-square-best50}& 50 features with highest $\chi^2$ value & 50\\
        7 & \textsf{ANOVA-best50} & 50 features with highest ANOVA F-scores & 50 \\
        8 & \textsf{MIG-best50} & 50 features with best MIG values & 50 \\
        9 & \textsf{ANOVA-wordvector34}& \textsf{Word2Vec} features that passed ANOVA& 34\\
    \hline
    \end{tabular}
    \caption{Different data views used for classifier training.}
    \label{tab:dataviews}
\end{table}

%% 3. Describe our evaluation procedure (10-fold CrossValidation)
\section{Classifier Training and Evaluation}

For each classifier -- data view pair
we used 10-fold Cross Validation across the entire training set of 433 data points
from both the official training set and the additional \textsf{annotated\_by\_1}
set, using a seeded stratified method provided by \textsf{scikit-learn}. For evaluating the pipelines, we performed a 10-fold Train \& Predict using the best hyper-parameters from the previous step across the organizer-recommended 325 training files. These predictions were eventually fed into the Ensemble Creation procedures. 

Scikit-Learn provides some utilities for hyper-parameter selection \textsf{RandomizedSearchCV} and \textsf{GridSearchCV} which allow an engineer to specify a parameter grid which will be either randomly sampled or exhaustively searched, respectively. The sheer number of parametric combinations for some pipelines, such as Bernoulli RBM followed by a SVM, were forced to use RandomizedSearch but GridSearch is preferred otherwise \cite{scikit-learn}.

Towards the end of the competition, we occasionally switched to using a pipeline trained on the 325 records from the training set, and predict the \textsf{Valence} of the 108 \textsf{annotated\_by\_1} dataset to make sure that there was no significant over-fitting (in addition to our usage of 10-fold cross validation). For every classifier except \textsf{XGBoost}, scores on the 108 \textsf{annotated\_by\_1} files were lower than the the 325 record test set.

\begin{table}[]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textsf{No.} & \textsf{Name} & \textsf{Algorithm} & \textsf{View} \\
    \hline
    1 & \textsf{MNB-CARs} & \textsf{MNB} & \textsf{CARs} \\
    2 & \textsf{RF-full} & \textsf{RF} & \textsf{Full} \\
    3 & \textsf{RF-reg-full} & \textsf{RF-reg} & \textsf{Full} \\
    4 & \textsf{RF-reg-clf-full} & \textsf{RF-reg-clf} & \textsf{Full} \\
    5 & \textsf{Lin-SVM-chi2-best} & \textsf{Lin-SVM} & \textsf{chi2-square-best50} \\
    6 & \textsf{Lin-SVM-anova-best} & \textsf{Lin-SVM} & \textsf{ANOVA-best50} \\
    7 & \textsf{RBF-SVR-mig-best} & \textsf{RBF-SVR} & \textsf{MIG-best50} \\
    8 & \textsf{SVM-Init-Ada-CARS} & \textsf{SVM-Init AdaBoost} & \textsf{Rules} \\
    9 & \textsf{D\&W-num} & \textsf{Deep \& Wide} & \textsf{Numeric} \\
    10 & \textsf{DNN-full} & \textsf{DNN} & \textsf{Full} \\
    \hline
    \end{tabular}
    \caption{Abbreviated names for classifiers for the next sections}
    \label{tab:Best6ClassifierNames}
    
\end{table}

{\renewcommand{\arraystretch}{0.75}%
\begin{table}[]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
             \hline
   \cellcolor{gray!15} \textsf{RF-reg-full} & \textsf{Pred NONE} & \textsf{Pred MILD} & \textsf{Pred MOD.} & \textsf{Pred SEVERE} \\
   \hline
   \textsf{True NONE} & \cellcolor{gray!15} 13 & 28 & 4 & 0 \\
   \textsf{True MILD} & 3 & \cellcolor{gray!15} 94 & 33 & 0 \\
   \textsf{True MODERATE} & 0 & 16 & \cellcolor{gray!15} 60 & 6 \\
   \textsf{True SEVERE} & 0 & 4 & 49 & \cellcolor{gray!15} 15 \\
   \hline
   \cellcolor{gray!15} \textsf{Lin-SVM-chi2-best} & \textsf{Pred NONE} & \textsf{Pred MILD} & \textsf{Pred MOD.} & \textsf{Pred SEVERE} \\
   \hline
   \textsf{True NONE} & \cellcolor{gray!15} 28 & 13 & 4 & 0 \\
   \textsf{True MILD} & 11 & \cellcolor{gray!15} 103 & 13 & 3 \\
   \textsf{True MODERATE} & 1 & 18 & \cellcolor{gray!15} 41 & 22 \\
   \textsf{True SEVERE} & 1 & 2 & 21 & \cellcolor{gray!15} 44 \\
   \hline
   \cellcolor{gray!15} \textsf{RBF-SVR-mig-best} & \textsf{Pred NONE} & \textsf{Pred MILD} & \textsf{Pred MOD.} & \textsf{Pred SEVERE} \\
   \hline
   \textsf{True NONE} & \cellcolor{gray!15} 17 & 25 & 3 & 0 \\
   \textsf{True MILD} & 10 & \cellcolor{gray!15} 90 & 30 & 0 \\
   \textsf{True MODERATE} & 1 & 26 & \cellcolor{gray!15} 43 & 12 \\
   \textsf{True SEVERE} & 1 & 1 & 47 & \cellcolor{gray!15} 19 \\
   \hline
   \cellcolor{gray!15} \textsf{D\&W-num} & \textsf{Pred NONE} & \textsf{Pred MILD} & \textsf{Pred MOD.} & \textsf{Pred SEVERE} \\
   \hline
   \textsf{True NONE} & \cellcolor{gray!15} 27 & 17 & 1 & 0 \\
   \textsf{True MILD} & 13 & \cellcolor{gray!15} 84 & 25 & 8 \\
   \textsf{True MODERATE} & 3 & 17 & \cellcolor{gray!15} 41 & 21 \\
   \textsf{True SEVERE} & 1 & 4 & 20 & \cellcolor{gray!15} 43 \\
   \hline
   \cellcolor{gray!15} \textsf{SVM-Init-Ada-CARS} & \textsf{Pred NONE} & \textsf{Pred MILD} & \textsf{Pred MOD.} & \textsf{Pred SEVERE} \\
   \hline
   \textsf{True NONE} & \cellcolor{gray!15} 30 & 2 & 13 & 0 \\
   \textsf{True MILD} & 9 & \cellcolor{gray!15} 96 & 21 & 4 \\
   \textsf{True MODERATE} & 1 & 16 & \cellcolor{gray!15} 58 & 7 \\
   \textsf{True SEVERE} & 0 & 2 & 14 & \cellcolor{gray!15} 52 \\
   \hline
   \cellcolor{gray!15} \textsf{MNB-CARs} & \textsf{Pred NONE} & \textsf{Pred MILD} & \textsf{Pred MOD.} & \textsf{Pred SEVERE} \\
   \hline
   \textsf{True NONE} & \cellcolor{gray!15} 31 & 14 & 0 & 0 \\
   \textsf{True MILD} & 16 & \cellcolor{gray!15} 103 & 3 & 8 \\
   \textsf{True MODERATE} & 5 & 33 & \cellcolor{gray!15} 34 & 10 \\
   \textsf{True SEVERE} & 1 & 6 & 5 & \cellcolor{gray!15} 56 \\
    \hline
    \end{tabular}
    \caption{Individual Confusion Matrices on the 325 document training set for the 6 Classifiers in the Competition-Winning Ensemble.}
    \label{tab:BestEnsembleClassifiersConfusionMatrix}
\end{table}} \quad


\begin{table}[]
    \centering
    \begin{tabular}{|l|c|c|c|l|c|c|c|}
          \hline
   \cellcolor{gray!15} \textsf{RF-reg-full} & \textsf{Prec.} & \textsf{Recall} & \textsf{MAE} & \cellcolor{gray!15} \textsf{D\&W-num} & \textsf{Prec.} & \textsf{Recall} & \textsf{MAE} \\
   \hline
    \textsf{NONE} & \cellcolor{gray!15} 0.812 & 0.289 & 0.733 & \textsf{NONE} & 0.614 & 0.600 & \cellcolor{gray!15} 0.859 \\
    \textsf{MILD} & 0.662 & 0.723 & \cellcolor{gray!15} 0.824 & \textsf{MILD} & \cellcolor{gray!15} 0.689 & \cellcolor{gray!15} 0.646 & 0.793 \\
    \textsf{MODERATE} & 0.411 & \cellcolor{gray!15} 0.732 & 0.809 & \textsf{MODERATE} & 0.471 & 0.500 & 0.732 \\
    \textsf{SEVERE} & 0.714 & 0.221 & 0.738 & \textsf{SEVERE} & 0.597 & 0.632 & 0.848 \\
   \hline
   \cellcolor{gray!15} \textsf{Lin-SVM-chi2} & \textsf{Prec.} & \textsf{Recall} & \textsf{MAE} & \cellcolor{gray!15} \textsf{SVM-Init} & \textsf{Prec.} & \textsf{Recall} & \textsf{MAE} \\
   \hline
    \textsf{NONE} & 0.683 & 0.622 & 0.844 & \textsf{NONE} & 0.750 & 0.667 & 0.793 \\
    \textsf{MILD} & \cellcolor{gray!15} 0.757 &\cellcolor{gray!15}  0.792 & \cellcolor{gray!15} 0.885 & \textsf{MILD} & \cellcolor{gray!15} 0.828 & 0.738 & 0.854 \\
    \textsf{MODERATE} & 0.519 & 0.500 & 0.744 & \textsf{MODERATE} & 0.547 & 0.707 & 0.848 \\
    \textsf{SEVERE} & 0.638 & 0.647 & 0.863 & \textsf{SEVERE} & 0.825 & \cellcolor{gray!15} 0.765 & \cellcolor{gray!15} 0.912 \\
   \hline
   \cellcolor{gray!15} \textsf{RBF-SVR-mig} & \textsf{Prec.} & \textsf{Recall} & \textsf{MAE} & \cellcolor{gray!15} \textsf{MNB-CARs} & \textsf{Prec.} & \textsf{Recall} & \textsf{MAE} \\
   \hline
    \textsf{NONE} & 0.586 & 0.378 & 0.757 & \textsf{NONE} & 0.585 & 0.689 & 0.896 \\
    \textsf{MILD} & \cellcolor{gray!15} 0.634 & \cellcolor{gray!15} 0.692 & \cellcolor{gray!15} 0.787 & \textsf{MILD} & 0.660 & \cellcolor{gray!15} 0.792 & 0.866 \\
    \textsf{MODERATE} & 0.350 & 0.524 & 0.745 & \textsf{MODERATE} & \cellcolor{gray!15} 0.810 & 0.415 & 0.677 \\
    \textsf{SEVERE} & 0.613 & 0.279 & 0.739 & \textsf{SEVERE} & 0.757 & 0.824 & \cellcolor{gray!15} 0.902 \\
   \hline
    \end{tabular}
    \caption{Multi-class Precision, Recall and MAE on the 325 document training set for the 6 Classifiers in the Competition-Winning Ensemble. Per-class MAE is normalized with the assumption that all predictions are maximally incorrect for each class.}
    \label{tab:BestEnsembleClassifiersMetrics}
\end{table}

\begin{table}[]
\centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textsf{ Classifier } & \textsf{ MA-MAE } & \textsf{ ROC-AUC } & \textsf{ $R^2$ } & \\
        \hline
      \cellcolor{gray!15} \textsf{ RF-reg-full } & 0.776 & 0.683 & \cellcolor{gray!15} 0.554 & \\
       \cellcolor{gray!15} \textsf{ Lin-SVM-chi2-best } & 0.834 & 0.765 & 0.521 & \\ 
       \cellcolor{gray!15} \textsf{ RBF-SVR-mig-best } & 0.757 & 0.657 & 0.476 & \\ 
       \cellcolor{gray!15} \textsf{ D\&W-num } & 0.808 & 0.721 & 0.394 & \\ 
       \cellcolor{gray!15} \textsf{ SVM-Init-Ada-CARS } & \cellcolor{gray!15} 0.851 & \cellcolor{gray!15} 0.811 & 0.515 & \\ 
       \cellcolor{gray!15} \textsf{ MNB-CARs } & 0.835 & 0.773 & 0.459 & \\
        \hline

    \textsf{ Classifier } & \textsf{ Precision } & \textsf{ Recall } & \textsf{ F1-Score } & \textsf{ Accuracy } \\ 
    \hline
    \cellcolor{gray!15} \textsf{ RF-reg-full } & 0.650 & 0.491 & 0.495 & 0.560 \\ 
   \cellcolor{gray!15} \textsf{ Lin-SVM-chi2-best } & 0.649 & 0.640 & 0.644 & 0.665 \\ 
   \cellcolor{gray!15} \textsf{ RBF-SVR-mig-best } & 0.546 & 0.468 & 0.481 & 0.520 \\ 
   \cellcolor{gray!15} \textsf{ D\&W-num } & 0.593 & 0.595 & 0.593 & 0.600 \\ 
   \cellcolor{gray!15} \textsf{ SVM-Init-Ada-CARS } & \cellcolor{gray!15} 0.738 & \cellcolor{gray!15} 0.719 & \cellcolor{gray!15} 0.724 & \cellcolor{gray!15} 0.726 \\ 
   \cellcolor{gray!15} \textsf{ MNB-CARs } & 0.703 & 0.680 & 0.673 & 0.689 \\
   \hline
    \end{tabular}
    \caption{Summary metrics on the 325 document training set for each of the 6 Classifiers in the Competition-Winning Ensemble. All applicable metrics are macro-averaged when necessary. Higher is better.}
    \label{tab:BestEnsembleClassifiersMAMetrics2}
\end{table}

Of the 154 total runs, we selected the 12 learners who scored above 0.60
to use in the next step: ensemble training \footnote{the exact threshold is coincidentally -- there was a large gap between 0.60 and a next-best cluster classifier+dataview combinations of scores around 0.54. Altogether, more than half of classifier+dataview combinations did only slightly better or equal to random}.  Of these 12 learners, 10 participated in the five \textit{best} ensembles (see Section \ref{sec:ensembles}.) These 10 are shown in Table \ref{tab:Best6ClassifierNames}
 which associates an abbreviated naming convention for each classfier+dataview.

For the sake of brevity, we limit the demonstration and discussion
of the results of the individual classifiers to the six
classifiers from Table \ref{tab:Best6ClassifierNames} which
constituted our top performing classification ensemble.
These classifiers are:
\begin{enumerate}
\item \textsf{RF-ref-full}: the Random Forest regression run on the \textsf{Full}
data view.
\item \textsf{Lin-SVM-chi2-best}: Linear kernel SVM classifier run
on the 50 best features selected by the  $\chi^2$ test.
\item \textsf{RBF-SVR-mig-best}: Radial basis function kernel SVM regressor run on
the top 50 best features selected by the mutual information gain test.
\item \textsf{D\&W-num}: the TensorFlow's Deep and Wide classifier run on
the numeric and categorical features.
\item \textsf{SVM-Init-Ada-CARs}: SVM-initialized Adaboost running on our CAR features.
\item \textsf{MNB-CARs}: Multinomial Na\"{i}ve Bayes running on our CAR features.

\end{enumerate}

Table \ref{tab:BestEnsembleClassifiersConfusionMatrix} contains the
confusion matrices for these six runs, Table \ref{tab:BestEnsembleClassifiersMetrics}
shows precision, recall and \textsf{MAE} for each class, while Table~ \ref{tab:BestEnsembleClassifiersMAMetrics2}
document the overall accuracy metrics: \textsf{MA-MAE}, \textsf{RoC-AUC}, precision, recall, f-score and accuracy.  We discuss the work of individual classifiers
below.

Our \textsf{RandomForest} regression run on the full data view (\textsf{RF-reg-full})
tended to over-predict \textsf{MILD} and \textsf{MODERATE} valences at the expense of
\textsf{NONE} and \textsf{SEVERE}, however, it contained excellent separation 
between the \textsf{NONE}/\textsf{MILD}, and \textsf{MODERATE}/\textsf{SEVERE} pairs
of valences, with only \textsf{MILD}$\Rightarrow$\textsf{MODERATE} false positives
being of concern.  While this run had the second lowest \textsf{MA-MAE} value out of our
six runs, it should be noted (see Section \ref{sec:ensembles}) that this is \textit{the only}
run that participated in all final ensembles. 

The linear kernel SVM classifier running on our top 50 $\chi^2$ features
(\textsf{Lin-SVM-chi2-best}) has the third highest \textsf{MA-MAE} and
has produced an excellent confusion matrix, with majority of \textsf{NONE}
and \textsf{SEVERE} conditions being classified correctly, and with very few
``costly" misses.

The SVM regressor with RBF kernel running on our top 50 Mutual Information Gain features
(\textsf{RBF-SVR-mig-best}) had the lowest performance of these six runs
(although was still among the better classifiers overall). It over-predicted
the \textsf{MODERATE} class, and had some trouble distinguishing \textsf{MODERATE}
and \textsf{MILD} valences. It also was very strict at predicting \textsf{NONE}
and \textsf{SEVERE} valences.

TensorFlow's  Deep and Wide  classifier, run on all our
numeric and categorical attributes, excluding CAR and \textsf{Word2Vec}
attributes (\textsf{D\&W-num}) had no significant distinctive features as compared to
other runs. It did the worst on properly capturing \textsf{MILD} valences
(\textsf{MILD} recall), and tended to admit more "big" mistakes
(misclassifications two or more classes apart) than some other methods. 
But it kept the overall number of misclassified cases reasonable,
and hence earned a \textsf{MA-MAE} in excess of 0.8.

Our overall best single run came from our own AdaBoost classifier
trained on a single round of SVM followed by 49 rounds of Na\"{i}ve
Bayes applied to the dataset consisting solely of \textsf{CAR} attributes
(\textsf{SVM-Init-Ada-CARs}, see Section \ref{sec:newml}). 
This classifier excelled almost everywhere, giving by
far the most accurate predictions of \textsf{SEVERE} valence and minimizing
false positives. The only ``weak spot" for this method came from improperly
classifying 13 cases with valence of \textsf{NONE} as \textsf{MODERATE}.
However, as this was a clear outlier prediction among our six runs (the other
runs predicted anywhere from 0 to 4 cases this way), this miss was effectively
eliminated in the followup ensembles.

The final classifier run, Multinomial Na\"{i}ve Bayes run on the same
data view of CAR attributes (\textsf{MNB-CARs}), edged the \textsf{Lin-SVM-chi2-best}
run by a hair to give us our second best single run \textsf{MA-MAE} of 0.835.
It got the largest number of both \textsf{NONE} and \textsf{SEVERE} true
positives, as well as tying the \textsf{Lin-SVM-chi2-best} for the
largest number of \textsf{MILD} true positives, only stumbling
a bit on the \textsf{MODERATE} valence, where it had a very high precision,
but low recall. 







